Looking for circuits in one to four layer language models - these are much smaller than real models and so much more tractable, but seem like they'll teach us useful insights! Previous work with two layer attention-only models found induction heads, which seem to recur in all models studied, and play a crucial role in detecting and learning long-range dependencies in text.\
Example: Can you reverse engineer the weights of an interpretable neuron in a one layer language model? (No one has done this, to my knowledge!)\
An example of a (seeming!) base64 neuron in a one layer model:\
